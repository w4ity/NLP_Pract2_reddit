{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T12:08:45.728897Z",
     "iopub.status.busy": "2025-06-19T12:08:45.728703Z",
     "iopub.status.idle": "2025-06-19T12:08:47.544547Z",
     "shell.execute_reply": "2025-06-19T12:08:47.543826Z",
     "shell.execute_reply.started": "2025-06-19T12:08:45.728875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/opiniones-polemicas/opiniones_polemicas.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T12:17:31.965434Z",
     "iopub.status.busy": "2025-06-19T12:17:31.964565Z",
     "iopub.status.idle": "2025-06-19T12:18:20.100001Z",
     "shell.execute_reply": "2025-06-19T12:18:20.099324Z",
     "shell.execute_reply.started": "2025-06-19T12:17:31.965408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec26bed80e5494bb71ca9d22496e5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecac4b76c654c608e77722d6d3c6937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d81eb2afec4366832a31fe2562fbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573070545b8742d897109bb49f32b755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8660fb854e8d42b29b0fa2e0a1ab848b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 12:17:36.783201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750335456.981513      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750335457.040414      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4a5d9d65ca4dc1bbaf1613aa068433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3d295d7d3646009112f2063322c86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b388be7e44e4c1db76192512f18c3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d68ed826c44ac594261e16b6ab4a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa671d49ad034543adcf33399ab645d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3036e7ec2aed47b4afad4613845b0602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo y tokenizer cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "archivo_token = \"../../tkn_huggingface_gema2b.txt\"\n",
    "\n",
    "# Leer el token desde el archivo\n",
    "try:\n",
    "    with open(archivo_token, \"r\", encoding=\"utf-8\") as f:\n",
    "        HF_TOKEN = f.read().strip()  # Elimina espacios y saltos de lÃ­nea\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{archivo_token}' no se encontrÃ³.\")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "model_name = \"google/gemma-2b\"\n",
    "\n",
    "# Cargar tokenizer y el modelo \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Usa GPU automÃ¡ticamente en Kaggle\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Modelo y tokenizer cargados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T12:20:32.483655Z",
     "iopub.status.busy": "2025-06-19T12:20:32.482995Z",
     "iopub.status.idle": "2025-06-19T12:33:49.045614Z",
     "shell.execute_reply": "2025-06-19T12:33:49.044893Z",
     "shell.execute_reply.started": "2025-06-19T12:20:32.483631Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clasificando hilos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [13:16<00:00, 79.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Resultados guardados en: clasificados_opiniones.json\n",
      "\n",
      "ðŸ§  AnÃ¡lisis de ejemplos clasificados:\n",
      "\n",
      "Comentario 1:\n",
      "Texto: Saludos, estimado /u/Migol-16, y usuarios de r/OpinionesPolemicas.\n",
      "\n",
      "Gracias por aportar nuevos e interesante y/o funables temas a nuestra comunidad, recuerda respetar las reglas para asÃ­ tener un mejo...\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 2:\n",
      "Texto: Poco se habla que los paÃ­ses Ã¡rabes principales (Egipto, Emiratos, Arabia) le tienen mÃ¡s tirria a IrÃ¡n que a Israel...\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 3:\n",
      "Texto: que consideramos discurso de odio? porque si cualquier cosa que diga sobre israel va a ser tachado de antisemita pues estamos jodidos...\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 4:\n",
      "Texto: Que muchos anglofilos critican al mundo Ã¡rabe de atrasado cuando los responsables son los gobiernos autoritarios y teocraricos sostenidos gracias a que detrÃ¡s hay una potencia respaldandolos, por ejem...\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 5:\n",
      "Texto: Los megathread, son la forma que tienen los mod de evitar post de cierto tipo....\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 6:\n",
      "Texto: [deleted]...\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 7:\n",
      "Texto: No hay duda de que IrÃ¡n estÃ¡ tratando de obtener armas nucleares, pueden estar a favor o en contra de los ataques de Israel, pero es un hecho innegable que los estÃ¡n haciendo, no le tienen que creer a...\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 8:\n",
      "Texto: Tengo hambre....\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 9:\n",
      "Texto: EstÃ¡n matando al sub y al planeta al no permitir voces disidentes que publiquen cosas que no os gusta, si no soportan las opiniones polemicas cambiaros el nombre a opiniones aceptables....\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n",
      "\n",
      "Comentario 10:\n",
      "Texto: El medio oriente se pelearÃ¡ entre sÃ­ (considerÃ³ Israel medio oriente y los judÃ­os bÃ¡sicamente Ã¡rabes) manipulados y coludidos con fuerzas externas, hasta el final de los tiempos. Nada mÃ¡s que decir. S...\n",
      "Zero-shot: SÃ­\n",
      "Few-shot: SÃ­\n",
      "Chain-of-thought: SÃ­\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_file = \"/kaggle/input/opiniones-polemicas/opiniones_polemicas.json\"\n",
    "output_file = \"clasificados_opiniones.json\"\n",
    "\n",
    "# Funcion de clasificacion\n",
    "def classify_comment(text, strategy=\"zero_shot\"):\n",
    "    \"\"\"Clasifica un comentario usando estrategias ZSL, FSL o Chain-of-thought\"\"\"\n",
    "    if strategy == \"zero_shot\":\n",
    "        prompt = f\"\"\"\n",
    "        Â¿El siguiente comentario contiene contenido sensible, ofensivo o inapropiado?\n",
    "        Responde Ãºnicamente con 'SÃ­' o 'No'.\n",
    "        Comentario: {text}\n",
    "        \"\"\"\n",
    "    elif strategy == \"few_shot\":\n",
    "        prompt = f\"\"\"\n",
    "        Ejemplos:\n",
    "        - Comentario: \"Odio a las personas de X paÃ­s.\" â†’ Respuesta: SÃ­\n",
    "        - Comentario: \"Me gusta el helado de chocolate.\" â†’ Respuesta: No\n",
    "        Â¿El siguiente comentario contiene contenido sensible, ofensivo o inapropiado?\n",
    "        Comentario: {text}\n",
    "        Responde Ãºnicamente con 'SÃ­' o 'No'.\n",
    "        \"\"\"\n",
    "    elif strategy == \"chain_of_thought\":\n",
    "        prompt = f\"\"\"\n",
    "        Analiza el siguiente comentario paso a paso:\n",
    "        1. Â¿Contiene lenguaje agresivo o discriminatorio?\n",
    "        2. Â¿Hace referencia a temas delicados como violencia, sexo o religiÃ³n de forma inadecuada?\n",
    "        3. Â¿Puede ser percibido como ofensivo por algÃºn grupo?\n",
    "        Finalmente, responde 'SÃ­' o 'No' segÃºn tu anÃ¡lisis.\n",
    "        Comentario: {text}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Estrategia no vÃ¡lida\")\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=1\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
    "        return \"SÃ­\" if \"sÃ­\" in response or \"si\" in response else \"No\"\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] ClasificaciÃ³n fallida: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Procesado principal\n",
    "def process_threads():\n",
    "    \"\"\"Carga datos desde archivo local, clasifica comentarios y guarda resultados\"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    strategies = [\"zero_shot\", \"few_shot\", \"chain_of_thought\"]\n",
    "\n",
    "    for thread in tqdm(data, desc=\"Clasificando hilos\"):\n",
    "        for comment in thread[\"comments\"]:\n",
    "            text = comment[\"comment\"]\n",
    "            for strategy in strategies:\n",
    "                comment[f\"clasificacion_{strategy}\"] = classify_comment(text, strategy)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… Resultados guardados en: {output_file}\")\n",
    "\n",
    "# Mostrar 10 ejemplos \n",
    "def analyze_results():\n",
    "    \"\"\"Muestra ejemplos de comentarios clasificados\"\"\"\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    examples = []\n",
    "    for thread in data:\n",
    "        for comment in thread[\"comments\"]:\n",
    "            if len(examples) >= 10:\n",
    "                break\n",
    "            examples.append(comment)\n",
    "\n",
    "    print(\"\\nðŸ§  AnÃ¡lisis de ejemplos clasificados:\")\n",
    "    for i, comment in enumerate(examples):\n",
    "        print(f\"\\nComentario {i+1}:\")\n",
    "        print(f\"Texto: {comment['comment'][:200]}...\")\n",
    "        print(f\"Zero-shot: {comment['clasificacion_zero_shot']}\")\n",
    "        print(f\"Few-shot: {comment['clasificacion_few_shot']}\")\n",
    "        print(f\"Chain-of-thought: {comment['clasificacion_chain_of_thought']}\")\n",
    "\n",
    "process_threads()\n",
    "analyze_results()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7697144,
     "sourceId": 12217661,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
